{"cells":[{"cell_type":"markdown","metadata":{"id":"oJ34uxMILElC"},"source":["#CMT309 – Data Science Portfolio (Spring)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1945,"status":"ok","timestamp":1707136427759,"user":{"displayName":"Oktay Karakuş","userId":"07357612204679807063"},"user_tz":0},"id":"7PRe1FadWJR-","outputId":"c1ebd857-11b7-4300-a501-5392f7060aa9"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\never\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\never\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping tokenizers\\punkt.zip.\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\never\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import string\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import math\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","# the code block below is directly downloading commentary.txt and superheros.csv into your drive folder. Please just run it and do not comment out.\n","from urllib import request\n","module_url = [f\"https://drive.google.com/uc?export=view&id=18y6hLv2bqAyJsIXwVCty58lF0u7yimVq\"]\n","name = ['commentary.txt']\n","for i in range(len(name)):\n","    with request.urlopen(module_url[i]) as f, open(name[i], 'w', encoding='utf-8') as outf:\n","        a = f.read()\n","        outf.write(a.decode('ISO-8859-1'))\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","import nltk\n","import re\n","from tqdm import tqdm\n","tqdm.pandas()\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"markdown","metadata":{"id":"gxiHUV1sdEjU"},"source":["# Q5) Text Analysis (25 marks)\n","\n","In this question, we will interrogate the football commentary dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ej3H78ubdEjV"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Minute</th>\n","      <th>Commentary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>97</td>\n","      <td>Plenty of chances in this game but neither tea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>97</td>\n","      <td>That's it! The referee blows the final whistle</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>97</td>\n","      <td>Ball possession: Tottenham: 44%, Liverpool: 56%.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>96</td>\n","      <td>James Milner relieves the pressure with a clea...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>96</td>\n","      <td>Poor play by Trent Alexander-Arnold as his wea...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Minute                                         Commentary\n","0      97  Plenty of chances in this game but neither tea...\n","1      97     That's it! The referee blows the final whistle\n","2      97   Ball possession: Tottenham: 44%, Liverpool: 56%.\n","3      96  James Milner relieves the pressure with a clea...\n","4      96  Poor play by Trent Alexander-Arnold as his wea..."]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('commentary.txt', sep='\\t')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"lM-Hl3i69JrI"},"source":["## Q5.1 - Preprocessing (2 marks)\n","\n","You must implement a method for obtaining tokenized, PoS-tagged and PoS-tagged and lemmatized versions of the Commentary column. You must use only `nltk` libraries. You must create 3 new columns: `Tokenized`, `PoS_tagged` and `PoS_lemmatized`, and create them in order:\n","\n","1.- New `Tokenized` column, by lower casing and tokenizing the `Commentary` column.\n","\n","2.- New `PoS_tagged` column, by pos_tagging the `Tokenized` column.\n","\n","3.- New `PoS_lemmatized` column, by lemmatizing only the words in the `PoS_tagged` column. The reason for doing it in this order is to present to the tagging function the original text.\n","\n","An example outcome of the returned data frame is given below for each columns first three rows:\n","\n","```python\n",">>print(df['Tokenized'][:3])\n","0    [plenty, of, chances, in, this, game, but, nei...\n","1    [that, 's, it, !, the, referee, blows, the, fi...\n","2    [ball, possession, :, tottenham, :, 44, %, ,, ...\n","Name: Tokenized, dtype: object\n","\n",">>print(df['PoS_tagged'][:3])\n","0    [(plenty, NN), (of, IN), (chances, NNS), (in, ...\n","1    [(that, DT), ('s, VBZ), (it, PRP), (!, .), (th...\n","2    [(ball, DT), (possession, NN), (:, :), (totten...\n","Name: PoS_tagged, dtype: object\n","\n",">>print(df['PoS_lemmatized'][:3])\n","0    [(plenty, NN), (of, IN), (chance, NNS), (in, I...\n","1    [(that, DT), ('s, VBZ), (it, PRP), (!, .), (th...\n","2    [(ball, DT), (possession, NN), (:, :), (totten...\n","Name: PoS_lemmatized, dtype: object\n","```"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Ix6Yi0c_P7E9"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\never\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\never\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\never\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["                                           Tokenized  \\\n","0  [plenty, of, chances, in, this, game, but, nei...   \n","1  [that, 's, it, !, the, referee, blows, the, fi...   \n","2  [ball, possession, :, tottenham, :, 44, %, ,, ...   \n","3  [james, milner, relieves, the, pressure, with,...   \n","4  [poor, play, by, trent, alexander-arnold, as, ...   \n","\n","                                          PoS_tagged  \\\n","0  [(plenty, NN), (of, IN), (chances, NNS), (in, ...   \n","1  [(that, DT), ('s, VBZ), (it, PRP), (!, .), (th...   \n","2  [(ball, DT), (possession, NN), (:, :), (totten...   \n","3  [(james, NNS), (milner, VBP), (relieves, VBZ),...   \n","4  [(poor, JJ), (play, NN), (by, IN), (trent, JJ)...   \n","\n","                                      PoS_lemmatized  \n","0  [(plenty, NN), (of, IN), (chance, NNS), (in, I...  \n","1  [(that, DT), ('s, VBZ), (it, PRP), (!, .), (th...  \n","2  [(ball, DT), (possession, NN), (:, :), (totten...  \n","3  [(james, NNS), (milner, VBP), (relieve, VBZ), ...  \n","4  [(poor, JJ), (play, NN), (by, IN), (trent, JJ)...  \n"]}],"source":["import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","# Load the data\n","df = pd.read_csv('commentary.txt', sep='\\t')\n","\n","# Define function for lemmatization\n","def get_wordnet_pos(treebank_tag):\n","    if treebank_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN\n","\n","def lemmatize_tokens(tokens):\n","    lemmatizer = WordNetLemmatizer()\n","    pos_tagged = pos_tag(tokens)\n","    lemmatized = []\n","    for word, tag in pos_tagged:\n","        wn_tag = get_wordnet_pos(tag)\n","        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n","        lemmatized.append((lemma, tag))\n","    return lemmatized\n","\n","# Tokenize the Commentary column\n","df['Tokenized'] = df['Commentary'].apply(lambda x: word_tokenize(x.lower()))\n","\n","# POS tagging\n","df['PoS_tagged'] = df['Tokenized'].apply(lambda x: pos_tag(x))\n","\n","# Lemmatization\n","df['PoS_lemmatized'] = df['Tokenized'].apply(lemmatize_tokens)\n","\n","# Display first few rows\n","print(df[['Tokenized', 'PoS_tagged', 'PoS_lemmatized']].head())\n"]},{"cell_type":"markdown","metadata":{"id":"IYsMTNgJWO-t"},"source":["## Q5.2 - Basic search engine (10 marks)\n","\n","In this question, we implement a basic search engine in a function called `retrieve_similar_commentaries(df, query, k)`, which takes as input the following arguments:\n","\n","- `df` the previously enriched (tokenized, pos tagged, etc) commentary dataframe.\n","- `query` a string of any type, which will be the query we will be using to retrieve similar commentaries.\n","- `k` and integer denoting the top `k` commentaries to be returned (by similarity).\n","\n","Our function must perform the following steps:\n","\n","1 - Tokenize and lemmatize the input query.\n","\n","2 - For each commentary in the df, compute how similar it is to the query as the number of shared tokens between query and commentary.\n","\n","3 - We will prioritize noun matches, so our similarity score will receive +1 if at least one of the matching tokens in the commentary is a noun (i.e., its part of speech starts with `N`). This means that, for example, if your query has 2 tokens, the maximum similarity a commentary can have is 4: 2 for 2 overlapping tokens, and 2 for both tokens being nouns.\n","\n","4 - The function must return a list of tuples of the form `[(commentary1, sim), (commentary2, sim) ... (commentaryk, sim)]`, where commentaries are ranked by `sim` value in descending order.\n","\n","An example test case is given below:\n","\n","```python\n",">>> result = retrieve_similar_commentaries(df, \"Manchester United ball\", 3)\n",">>> for idx,r in enumerate(result):\n",">>>   print(idx,r)\n","\n","0 ('Manchester United is in control of the ball.', 5)\n","1 ('Manchester United is in control of the ball.', 5)\n","2 ('Jadon Sancho from Manchester United crosses the ball, but it goes out for a goal kick.', 5)\n","```"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"QTj15d4bQSl0"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 ('ball possession : newcastle united : 28 % , manchester city : 72 % .', 5)\n","1 ('ball possession : newcastle united : 28 % , manchester city : 72 % .', 5)\n","2 ('ball possession : newcastle united : 29 % , manchester city : 71 % .', 5)\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","def retrieve_similar_commentaries(df, query, k):\n","    # Tokenize and lemmatize the input query\n","    lemmatizer = WordNetLemmatizer()\n","    query_tokens = [lemmatizer.lemmatize(token) for token in word_tokenize(query.lower())]\n","    \n","    # Compute similarity for each commentary\n","    similarities = []\n","    for commentary in df['Tokenized']:\n","        # Convert commentary list to string\n","        commentary_str = ' '.join(commentary)\n","        commentary_tokens = word_tokenize(commentary_str.lower())\n","        shared_tokens = set(query_tokens).intersection(commentary_tokens)\n","        \n","        # Compute similarity score\n","        similarity = len(shared_tokens)\n","        for token in shared_tokens:\n","            if nltk.pos_tag([token])[0][1].startswith('N'):\n","                similarity += 1\n","        \n","        similarities.append((commentary_str, similarity))\n","    \n","    # Sort commentaries by similarity score in descending order\n","    similarities.sort(key=lambda x: x[1], reverse=True)\n","    \n","    # Return top k similar commentaries\n","    return similarities[:k]\n","\n","# Test case\n","result = retrieve_similar_commentaries(df, \"Manchester United ball\", 3)\n","for idx, r in enumerate(result):\n","    print(idx, r)\n"]},{"cell_type":"markdown","metadata":{"id":"xdbA8XlR9j6P"},"source":["## Q5.3 - PMI (13 marks)\n","\n","In this question, you implement and apply the pointwise mutual information (PMI) metric, a word association metric introduced in 1992, to the football commentaries. The purpose of PMI is to extract, from free text, pairs of words or phrases than tend to co-occur together more often than expected by chance. For example, PMI(`new`, `york`) would give a higher score than PMI(`new`, `car`) because the chance of finding `new` and `york` together in text is higher than `new` and `car`, despite `new` being a more frequent word than `york`.\n","\n","The formula for PMI (where `x` and `y` are two words) is:\n","\n","$PMI(x,y) = log(\\frac{p(x,y)}{p(x)p(y)})$\n","\n","Watch this video to understand how to estimate these probabilities: https://www.youtube.com/watch?v=swDoFpuHpzQ."]},{"cell_type":"markdown","metadata":{"id":"zjwmwOLHMyB9"},"source":["Detailed instructions:\n","\n","You will implement the following logic:\n","\n","- **Phrase Extraction**: The first step is to extract noun phrases (NPs) and verb phrases (VPs) from the lemmatized data. To do this, you'll need to write a function that goes through each entry and groups words into noun phrases or verb phrases based on their part-of-speech tags. We will reward cases where NPs and VPs go beyond single word matching.\n","\n","- **Phrase Counting**: Once you have extracted the NPs and VPs, you'll need to count how many times each phrase occurs in the dataset. You'll have to write a function that iterates through the NPs and VPs and keeps track of the counts in dictionaries.\n","\n","- **Total Counts**: The next step is to compute the total count of all NPs and VPs. This is simply the sum of all the counts in the dictionaries you created in the previous step.\n","\n","- **Identifying Top Phrases**: To reduce computational complexity, we only want to compute PMI for the top occurring NPs and VPs. So, you will need to write a function that sorts the phrases by their counts and selects the top 100 phrases.\n","\n","- **Creating the PMI Matrix**: Finally, you'll create a PMI matrix using the top NPs and VPs, their counts, and the total counts of NPs and VPs. This matrix will be a pandas DataFrame, which will have rows corresponding to the top VPs, columns corresponding to the top NPs, and each cell will contain the PMI value between the corresponding NP and VP. This part of your solution will return 0 when there is no co-occurrence between an NP and a VP, and apply smoothing only to the final PMI value (refer to the video).\n","\n","You must implement all the functionality in a function called `compute_pmi_dataframe(df)` that takes as input the enriched `df` you created in `Q5.1`. You are encouraged to implement additional functions to break down your code and make it clear how you are separating different functionalities.\n","\n","An example test case is given below:\n","\n","```python\n",">>> def top_k_vps(pmi_matrix, np, k):\n",">>>    # Check if the NP exists in the matrix\n",">>>    if np in pmi_matrix.T.index:\n",">>>        top_vps = pmi_matrix.T.loc[np].nlargest(k)\n",">>>        return top_vps.index.tolist()\n",">>>    else:\n",">>>        print(f\"Noun phrase '{np}' not found in PMI matrix.\")\n",">>>        return []\n",">>>top_k_vps(pmidf, 'joao cancelo', 3)\n","\n","['arrives', 'cut', 'benefit']\n","```"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"nwjShxK2RpGo"},"outputs":[{"name":"stdout","output_type":"stream","text":["Noun Phrases: ['plenty', 'chances', 'game', 'team', 'score']\n","Verb Phrases: ['be', 'minded joseph gomez', 'be replaced', 'goes', 'are']\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import defaultdict\n","\n","def extract_phrases(pos_tagged):\n","    phrases = []\n","    current_phrase = []\n","    for word, pos in pos_tagged:\n","        if pos.startswith('N') or pos.startswith('V'):\n","            current_phrase.append(word)\n","        else:\n","            if current_phrase:\n","                phrases.append(' '.join(current_phrase))\n","                current_phrase = []\n","    if current_phrase:\n","        phrases.append(' '.join(current_phrase))\n","    return phrases\n","\n","def extract_np_vp(df):\n","    np_phrases = []\n","    vp_phrases = []\n","    for pos_tagged in df['PoS_tagged']:\n","        phrases = extract_phrases(pos_tagged)\n","        for phrase in phrases:\n","            if nltk.pos_tag(word_tokenize(phrase))[0][1].startswith('N'):\n","                np_phrases.append(phrase)\n","            elif nltk.pos_tag(word_tokenize(phrase))[0][1].startswith('V'):\n","                vp_phrases.append(phrase)\n","    return np_phrases, vp_phrases\n","\n","\n","\n","# Test case\n","np_phrases, vp_phrases = extract_np_vp(df)\n","print(\"Noun Phrases:\", np_phrases[:5])\n","print(\"Verb Phrases:\", vp_phrases[:5])\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1707136567731,"user":{"displayName":"Oktay Karakuş","userId":"07357612204679807063"},"user_tz":0},"id":"fo9ahEkyJlH2","outputId":"209aca0c-36c7-4fac-ddf0-2840a9385105"},"outputs":[{"ename":"NameError","evalue":"name 'pmidf' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoun phrase \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in PMI matrix.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m---> 10\u001b[0m top_k_vps(\u001b[43mpmidf\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoao cancelo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'pmidf' is not defined"]}],"source":["# you can test your resulting matrix\n","def top_k_vps(pmi_matrix, np, k):\n","    # Check if the NP exists in the matrix\n","    if np in pmi_matrix.T.index:\n","        top_vps = pmi_matrix.T.loc[np].nlargest(k)\n","        return top_vps.index.tolist()\n","    else:\n","        print(f\"Noun phrase '{np}' not found in PMI matrix.\")\n","        return []\n","top_k_vps(pmidf, 'joao cancelo', 3)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMONo6XtQB5ELJcx6+88za1","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":0}
